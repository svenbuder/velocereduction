{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c53d78",
   "metadata": {},
   "source": [
    "# VeloceReduction -- Tutorial\n",
    "\n",
    "This tutorial provides an example on how to reduce data of a given night YYMMDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b08276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic packages\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# VeloceReduction modules and function\n",
    "from velocereduction import config\n",
    "import velocereduction as VR\n",
    "\n",
    "from astropy.table import Table\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28891ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_memory = VR.utils.get_memory_usage()\n",
    "print('Memory before starting the reduction:')\n",
    "print(starting_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982f57a",
   "metadata": {},
   "source": [
    "## Adjust Date and Directory (possibly via argument parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba063e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    # Create the parser\n",
    "    parser = argparse.ArgumentParser(description=\"Process some inputs.\")\n",
    "    \n",
    "    # Add arguments\n",
    "    parser.add_argument('-d','--date', type=str, default=\"001122\",\n",
    "                        help='Date in the format DDMMYY (e.g., \"001122\")')\n",
    "    parser.add_argument('-wd','--working_directory', type=str, default=\"./\",\n",
    "                        help='The directory where the script will operate.')\n",
    "    \n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def get_script_input():\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        \n",
    "        # Assume default values if inside Jupyter\n",
    "        jupyter_date = \"001122\"\n",
    "        \n",
    "        # 2Amp example\n",
    "#         jupyter_date = \"240219\"\n",
    "        \n",
    "        # 4Amp example\n",
    "#         jupyter_date = \"231121\"\n",
    "\n",
    "        jupyter_working_directory = \"../\"\n",
    "        print(\"Running in a Jupyter notebook. Using predefined values\")\n",
    "        args = argparse.Namespace(date=jupyter_date, working_directory=jupyter_working_directory)\n",
    "    else:\n",
    "        # Use argparse to handle command-line arguments\n",
    "        print(\"Running as a standalone Python script\")\n",
    "        args = parse_arguments()\n",
    "\n",
    "    return args\n",
    "\n",
    "# Use the function to get input\n",
    "args = get_script_input()\n",
    "config.date = args.date\n",
    "\n",
    "if args.working_directory[:2] in ['./','~/']:\n",
    "    config.working_directory = str(Path(args.working_directory).resolve())+'/'\n",
    "else:\n",
    "    config.working_directory = str(Path(args.working_directory).resolve())+'/'\n",
    "print(f\"Date: {config.date}, Working Directory: {config.working_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4baf7e",
   "metadata": {},
   "source": [
    "## Identfiy Calibration and Science Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0c948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the Calibration and Science data from the night log\n",
    "calibration_runs, science_runs = VR.utils.identify_calibration_and_science_runs(\n",
    "    config.date,\n",
    "    config.working_directory+'observations/',\n",
    "    each_science_run_separately = False # Set this True, if you want to reduce the runs of the same object separately\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ac5ac",
   "metadata": {},
   "source": [
    "## Extract orders and save in initial FITS files with an extension per order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cccdc",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract Master Flat\n",
    "print('\\nExtracting Master Flat')\n",
    "master_flat, master_flat_images = VR.extraction.extract_orders(\n",
    "    ccd1_runs = calibration_runs['Flat_60.0'],\n",
    "    ccd2_runs = calibration_runs['Flat_1.0'],\n",
    "    ccd3_runs = calibration_runs['Flat_0.1'],\n",
    "    Flat = True,\n",
    "    update_tramlines_based_on_flat = True, # Would update and overwrite\n",
    "    # ./VeloceReduction/tramline_information/tramline_begin_end_ccd_*_oder_*.txt\n",
    "    debug_overscan = False,\n",
    "    debug_rows = False, # Plotting the distribution of counts of a tramline in every 500th row\n",
    "    debug_tramlines = True # Would create a tramlines trace PDF under\n",
    "    # reduced_data/YYMMDD/debug/debug_tramlines_flat.pdf\n",
    ")\n",
    "\n",
    "# Extract Master ThXe\n",
    "print('\\nExtracting Master ThXe')\n",
    "master_thxe = VR.extraction.extract_orders(\n",
    "    ccd1_runs = calibration_runs['FibTh_180.0'],\n",
    "    ccd2_runs = calibration_runs['FibTh_60.0'],\n",
    "    ccd3_runs = calibration_runs['FibTh_15.0'],\n",
    "    ThXe = True,\n",
    "    master_flat_images = master_flat_images,\n",
    "    debug_tramlines = True # Would create a tramlines trace PDF under\n",
    "    # reduced_data/YYMMDD/debug/debug_tramlines_thxe.pdf\n",
    ")\n",
    "\n",
    "# Extract Master LC\n",
    "if len(calibration_runs['SimLC']) > 0:\n",
    "    print('\\nExtracting Master LC')\n",
    "    master_lc = VR.extraction.extract_orders(\n",
    "        ccd1_runs = calibration_runs['SimLC'],\n",
    "        ccd2_runs = calibration_runs['SimLC'],\n",
    "        ccd3_runs = calibration_runs['SimLC'],\n",
    "        LC = True,\n",
    "        master_flat_images = master_flat_images,\n",
    "        debug_tramlines = True # Would create a tramlines trace PDF under\n",
    "        # reduced_data/YYMMDD/debug/debug_tramlines_lc.pdf\n",
    "    )\n",
    "else:\n",
    "    print('\\nNo SimLC observed for this night; Skipping Master LC calibration and setting all values 0.0')\n",
    "    master_lc = master_thxe; master_lc[:] = 0.0\n",
    "\n",
    "# Extract Darks\n",
    "master_darks = dict()\n",
    "if len(calibration_runs['Darks']) > 0:\n",
    "    print('\\nExtracting Darks')\n",
    "    for dark_exposure in calibration_runs['Darks'].keys():\n",
    "        print('  --> '+str(dark_exposure)+': '+','.join(calibration_runs['Darks'][dark_exposure]))\n",
    "        master_darks[dark_exposure] = VR.extraction.get_master_dark(calibration_runs['Darks'][dark_exposure])\n",
    "else:\n",
    "    print('\\nNo Dark exposure found for '+config.date+'. Using Archvial exposure from 001122 (2Amp.)')\n",
    "    master_darks['1800.0'] = VR.extraction.get_master_dark(calibration_runs['Darks'], archival=True)\n",
    "\n",
    "# Extract BStars -> Telluric lines\n",
    "master_bstars = dict()\n",
    "if len(calibration_runs['Bstar']) > 0:\n",
    "    print('\\nExtracting Bstar-Tellurics')\n",
    "    for bstar_exposure in calibration_runs['Bstar'].keys():\n",
    "        print('  --> '+str(bstar_exposure)+': '+', '.join(calibration_runs['Bstar'][bstar_exposure]))\n",
    "        telluric_flux, telluric_mjd = VR.extraction.get_tellurics_from_bstar(\n",
    "            calibration_runs['Bstar'][bstar_exposure], master_flat_images\n",
    "        )\n",
    "        master_bstars[telluric_mjd] = telluric_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56105fe5",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract Science Objects and save them into FITS files under reduced_data/\n",
    "for science_object in list(science_runs.keys()):\n",
    "    print('\\nExtracting '+science_object)\n",
    "\n",
    "#     try:\n",
    "    science, science_noise, science_header = VR.extraction.extract_orders(\n",
    "        ccd1_runs = science_runs[science_object],\n",
    "        ccd2_runs = science_runs[science_object],\n",
    "        ccd3_runs = science_runs[science_object],\n",
    "        Science=True,\n",
    "        master_darks = master_darks, # These are needed to subtract the dark current\n",
    "        master_flat_images = master_flat_images, # These are needed for flat-field correction\n",
    "        debug_tramlines = True, # Would create a tramlines trace PDF under\n",
    "        # reduced_data/YYMMDD/debug/debug_tramlines_{metadata['OBJECT']}.pdf\n",
    "        debug_overscan=False\n",
    "    )\n",
    "\n",
    "    # Find the closest BStar calibration\n",
    "    if len(master_bstars) > 0:\n",
    "        mjd_science = science_header['UTMJD']\n",
    "        mjd_tellurics = np.array(list(master_bstars.keys()))\n",
    "        closest_tellurics = mjd_tellurics[np.argmin(np.abs(mjd_tellurics - mjd_science))]\n",
    "        print(f'  --> Using telluric from B Star as observed at UTMJD {closest_tellurics} (Science taken at UTMJD {mjd_science})')\n",
    "        telluric = master_bstars[closest_tellurics]\n",
    "    else:\n",
    "        print('No tellurics from B Stars available.')\n",
    "        telluric = np.ones(np.shape(science))\n",
    "\n",
    "    # Create a primary HDU and HDU list\n",
    "    primary_hdu = fits.PrimaryHDU()\n",
    "    header = primary_hdu.header\n",
    "    header['OBJECT']             = (science_object,           'Name of observed object in night log')\n",
    "    header['UTMJD']              = (science_header['UTMJD'],  'Modified Julian Date of observation')\n",
    "    header['MEANRA']             = (science_header['MEANRA'], 'Mean Right Ascension of observed object')\n",
    "    header['MEANDEC']            = (science_header['MEANDEC'],'Mean Declination of observed object')        \n",
    "    header['BARYVEL']            = (0.0,                      'Applied barycentric velocity correction')\n",
    "    header['VRAD']               = ('None',                   'Radial velocity estimate')\n",
    "    header['E_VRAD']             = ('None',                   'Uncertainty of radial velocity estimate')\n",
    "\n",
    "    # Use astroquery to update header with Simbad information (where available)\n",
    "    # We try to find matches with HIP/2MASS/Gaia DR3 as well as\n",
    "    # radial velocities (VRAD), stellar parameters (TEFF/LOGG/FE_H), and \n",
    "    # magnitudes in B/V/G/R as well as parallax PLX\n",
    "    header = VR.utils.update_fits_header_via_crossmatch_with_simbad(header)\n",
    "\n",
    "    hdul = fits.HDUList([primary_hdu])\n",
    "\n",
    "    # Extract order ranges and coefficients\n",
    "    order_ranges, order_beginning_coeffs, order_ending_coeffs = VR.extraction.read_in_order_tramlines()\n",
    "\n",
    "    # Loop over your extension names and corresponding data arrays\n",
    "    for ext_index, ext_name in enumerate(order_beginning_coeffs):\n",
    "        # Create an ImageHDU object for each extension\n",
    "\n",
    "        # Define the columns with appropriate formats\n",
    "        col1_def = fits.Column(name='wave_vac',format='E', array=np.arange(len(science[ext_index,:]),dtype=float))\n",
    "        col2_def = fits.Column(name='wave_air',format='E', array=np.arange(len(science[ext_index,:]),dtype=float))\n",
    "        col3_def = fits.Column(name='science', format='E', array=science[ext_index,:])\n",
    "        col4_def = fits.Column(name='science_noise',   format='E', array=science_noise[ext_index,:])\n",
    "        col5_def = fits.Column(name='flat',    format='E', array=master_flat[ext_index,:])\n",
    "        col6_def = fits.Column(name='thxe',    format='E', array=master_thxe[ext_index,:])\n",
    "        col7_def = fits.Column(name='lc',      format='E', array=master_lc[ext_index,:])\n",
    "        col8_def = fits.Column(name='telluric',format='E', array=telluric[ext_index,:])\n",
    "\n",
    "        # Combine columns to BinTable and add header from primary\n",
    "        hdu = fits.BinTableHDU.from_columns([col1_def, col2_def, col3_def, col4_def, col5_def, col6_def, col7_def, col8_def], name=ext_name.lower())\n",
    "\n",
    "        # Append the HDU to the HDU list\n",
    "        hdul.append(hdu)\n",
    "\n",
    "    # Save to a new FITS file with an extension for each order\n",
    "    Path(config.working_directory+'reduced_data/'+config.date+'/'+science_object).mkdir(parents=True, exist_ok=True)\n",
    "    spectrum_filename = 'veloce_spectra_'+science_object+'_'+config.date+'.fits'\n",
    "    hdul.writeto(config.working_directory+'reduced_data/'+config.date+'/'+science_object+'/'+spectrum_filename, overwrite=True)\n",
    "\n",
    "    print('\\n  --> Successfully extracted '+science_object)\n",
    "\n",
    "#     except:\n",
    "#         print('\\n  --> Failed to extract '+science_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bf52b",
   "metadata": {},
   "source": [
    "## Wavelength calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b6d6f",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for science_object in list(science_runs.keys()):\n",
    "#     try:\n",
    "    VR.calibration.calibrate_wavelength(\n",
    "        science_object,\n",
    "        optimise_lc_solution=False,\n",
    "        correct_barycentric_velocity=True,\n",
    "        create_overview_pdf=True\n",
    "    )\n",
    "#         print('  -> Succesfully calibrated wavelength with diagnostic plots for '+science_object+'\\n')\n",
    "#     except:\n",
    "#         print('  -> Failed to calibrate wavelength for '+science_object+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c7d27",
   "metadata": {},
   "source": [
    "## Comparison with synthetic spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf29626",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for science_object in list(science_runs.keys()):\n",
    "    \n",
    "    print('\\nCalibrating wavelength for '+science_object+' with given radial velocity and synthetic Korg spectrum')\n",
    "    \n",
    "    with fits.open(config.working_directory+'reduced_data/'+config.date+'/'+science_object+'/veloce_spectra_'+science_object+'_'+config.date+'.fits', mode='update') as veloce_fits_file:\n",
    "        \n",
    "        korg_spectra = VR.flux_comparison.read_available_korg_syntheses()\n",
    "        \n",
    "        # Find the closest match based on (possibly available) literature TEFF/LOGG/FE_H\n",
    "        closest_korg_spectrum = VR.utils.find_closest_korg_spectrum(\n",
    "            available_korg_spectra = korg_spectra,\n",
    "            fits_header = veloce_fits_file[0].header,\n",
    "        )\n",
    "\n",
    "        # Find the best RV or raise ValueError of none available.\n",
    "        vrad_for_calibration = VR.utils.find_best_radial_velocity_from_fits_header(fits_header = veloce_fits_file[0].header)\n",
    "\n",
    "        # Let's test this for a few orders (or simply set order_selection = None to use all valid ones)\n",
    "        orders_to_calibrate = ['ccd_3_order_94','ccd_3_order_89']\n",
    "\n",
    "        VR.flux_comparison.calculate_wavelength_coefficients_with_korg_synthesis(\n",
    "            veloce_fits_file,\n",
    "            korg_wavelength_vac = korg_spectra['wavelength_vac'],\n",
    "            korg_flux = korg_spectra['flux_'+closest_korg_spectrum],\n",
    "            vrad_for_calibration = vrad_for_calibration,\n",
    "            order_selection=orders_to_calibrate,\n",
    "            telluric_hinkle_or_bstar = 'hinkle', # You can choose between 'hinkle' and 'bstar'\n",
    "            debug=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a453bc0",
   "metadata": {},
   "source": [
    "## Monitor RV (for stars with multiple observations and seperate reductions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3eb4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "repeated_observations = VR.utils.check_repeated_observations(science_runs)\n",
    "\n",
    "VR.utils.monitor_vrad_for_repeat_observations(config.date, repeated_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675269d1",
   "metadata": {},
   "source": [
    "## Final Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eac440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Memory before starting the reduction was:')\n",
    "print(starting_memory)\n",
    "print('Memory after running the reduction is:')\n",
    "print(VR.utils.get_memory_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864735f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13584d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
